<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Ivan Sopov"><meta name=description content="In the previous post I&rsquo;ve tried to measure overhead from highly contended access to sync.Pool on go. There was some measurable overhead, but it was mild. However in order for another goroutine to recieve some work to be done or/and to publish work results you may need go channel also. In this post I&rsquo;ll try to measure the overhead of contended access to go channel.
So I&rsquo;ve written the following code"><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Go channel(s) contention"><meta name=twitter:description content="In the previous post I&rsquo;ve tried to measure overhead from highly contended access to sync.Pool on go. There was some measurable overhead, but it was mild. However in order for another goroutine to recieve some work to be done or/and to publish work results you may need go channel also. In this post I&rsquo;ll try to measure the overhead of contended access to go channel.
So I&rsquo;ve written the following code"><meta property="og:title" content="Go channel(s) contention"><meta property="og:description" content="In the previous post I&rsquo;ve tried to measure overhead from highly contended access to sync.Pool on go. There was some measurable overhead, but it was mild. However in order for another goroutine to recieve some work to be done or/and to publish work results you may need go channel also. In this post I&rsquo;ll try to measure the overhead of contended access to go channel.
So I&rsquo;ve written the following code"><meta property="og:type" content="article"><meta property="og:url" content="https://isopov.github.io/posts/go-channel-contention/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-27T20:22:39+03:00"><meta property="article:modified_time" content="2023-03-27T20:22:39+03:00"><title>Go channel(s) contention Â· isopov</title><link rel=canonical href=https://isopov.github.io/posts/go-channel-contention/><link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.d9fddbffe6f27e69985dc5fe0471cdb0e57fbf4775714bc3d847accb08f4a1f6.css integrity="sha256-2f3b/+byfmmYXcX+BHHNsOV/v0d1cUvD2Eesywj0ofY=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.002ee2378e14c7a68f1f0a53d9694ed252090987c4e768023fac694a4fc5f793.css integrity="sha256-AC7iN44Ux6aPHwpT2WlO0lIJCYfE52gCP6xpSk/F95M=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><meta name=generator content="Hugo 0.119.0"></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>isopov</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://isopov.github.io/posts/go-channel-contention/>Go channel(s) contention</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2023-03-27T20:22:39+03:00>March 27, 2023</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
3-minute read</span></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/golang/>Golang</a></span></div></div></header><div><p>In the <a href=https://isopov.github.io/posts/sync-pool-contention/>previous post</a> I&rsquo;ve tried to measure overhead from highly contended access to sync.Pool on go. There was some measurable overhead, but it was mild. However in order for another goroutine to recieve some work to be done or/and to publish work results you may need go channel also. In this post I&rsquo;ll try to measure the overhead of contended access to go channel.</p><p>So I&rsquo;ve written the following code</p><div class=highlight><pre tabindex=0 style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#fff;font-weight:700>package</span> main
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#fff;font-weight:700>import</span> (
</span></span><span style=display:flex><span>	<span style=color:#0ff;font-weight:700>&#34;runtime&#34;</span>
</span></span><span style=display:flex><span>	<span style=color:#0ff;font-weight:700>&#34;strconv&#34;</span>
</span></span><span style=display:flex><span>	<span style=color:#0ff;font-weight:700>&#34;sync/atomic&#34;</span>
</span></span><span style=display:flex><span>	<span style=color:#0ff;font-weight:700>&#34;testing&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#fff;font-weight:700>const</span> workSize = <span style=color:#ff0;font-weight:700>1000</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#fff;font-weight:700>func</span> workWithChan(ch <span style=color:#fff;font-weight:700>chan</span> <span style=color:#fff;font-weight:700>int</span>) {
</span></span><span style=display:flex><span>	<span style=color:#fff;font-weight:700>for</span> i := <span style=color:#ff0;font-weight:700>0</span>; i &lt; workSize; i++ {
</span></span><span style=display:flex><span>		<span style=color:#fff;font-weight:700>select</span> {
</span></span><span style=display:flex><span>		<span style=color:#fff;font-weight:700>case</span> ch &lt;- i:
</span></span><span style=display:flex><span>		<span style=color:#fff;font-weight:700>default</span>:
</span></span><span style=display:flex><span>			<span style=color:#fff;font-weight:700>panic</span>(<span style=color:#0ff;font-weight:700>&#34;chan is full&#34;</span>)
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	<span style=color:#fff;font-weight:700>for</span> i := <span style=color:#ff0;font-weight:700>0</span>; i &lt; workSize; i++ {
</span></span><span style=display:flex><span>		<span style=color:#fff;font-weight:700>select</span> {
</span></span><span style=display:flex><span>		<span style=color:#fff;font-weight:700>case</span> &lt;-ch:
</span></span><span style=display:flex><span>		<span style=color:#fff;font-weight:700>default</span>:
</span></span><span style=display:flex><span>			<span style=color:#fff;font-weight:700>panic</span>(<span style=color:#0ff;font-weight:700>&#34;chan is empty&#34;</span>)
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#fff;font-weight:700>func</span> Benchmark_Chan(b *testing.B) {
</span></span><span style=display:flex><span>	<span style=color:#fff;font-weight:700>for</span> _, parallelism := <span style=color:#fff;font-weight:700>range</span> []<span style=color:#fff;font-weight:700>int</span>{<span style=color:#ff0;font-weight:700>1</span>, <span style=color:#ff0;font-weight:700>2</span>, <span style=color:#ff0;font-weight:700>3</span>, <span style=color:#ff0;font-weight:700>4</span>, <span style=color:#ff0;font-weight:700>5</span>, <span style=color:#ff0;font-weight:700>10</span>, <span style=color:#ff0;font-weight:700>20</span>, <span style=color:#ff0;font-weight:700>50</span>, <span style=color:#ff0;font-weight:700>100</span>} {
</span></span><span style=display:flex><span>		b.Run(<span style=color:#0ff;font-weight:700>&#34;Parallelism &#34;</span>+strconv.Itoa(parallelism), <span style=color:#fff;font-weight:700>func</span>(b *testing.B) {
</span></span><span style=display:flex><span>			b.Run(<span style=color:#0ff;font-weight:700>&#34;one chan&#34;</span>, <span style=color:#fff;font-weight:700>func</span>(b *testing.B) {
</span></span><span style=display:flex><span>				b.SetParallelism(parallelism)
</span></span><span style=display:flex><span>				ch := <span style=color:#fff;font-weight:700>make</span>(<span style=color:#fff;font-weight:700>chan</span> <span style=color:#fff;font-weight:700>int</span>, workSize*parallelism*runtime.GOMAXPROCS(<span style=color:#ff0;font-weight:700>0</span>))
</span></span><span style=display:flex><span>				b.RunParallel(<span style=color:#fff;font-weight:700>func</span>(pb *testing.PB) {
</span></span><span style=display:flex><span>					<span style=color:#fff;font-weight:700>for</span> pb.Next() {
</span></span><span style=display:flex><span>						workWithChan(ch)
</span></span><span style=display:flex><span>					}
</span></span><span style=display:flex><span>				})
</span></span><span style=display:flex><span>			})
</span></span><span style=display:flex><span>			b.Run(<span style=color:#0ff;font-weight:700>&#34;chan per core&#34;</span>, <span style=color:#fff;font-weight:700>func</span>(b *testing.B) {
</span></span><span style=display:flex><span>				b.SetParallelism(parallelism)
</span></span><span style=display:flex><span>				numChans := runtime.GOMAXPROCS(<span style=color:#ff0;font-weight:700>0</span>)
</span></span><span style=display:flex><span>				chans := <span style=color:#fff;font-weight:700>make</span>([]<span style=color:#fff;font-weight:700>chan</span> <span style=color:#fff;font-weight:700>int</span>, numChans)
</span></span><span style=display:flex><span>				<span style=color:#fff;font-weight:700>for</span> i := <span style=color:#ff0;font-weight:700>0</span>; i &lt; numChans; i++ {
</span></span><span style=display:flex><span>					chans[i] = <span style=color:#fff;font-weight:700>make</span>(<span style=color:#fff;font-weight:700>chan</span> <span style=color:#fff;font-weight:700>int</span>, workSize*parallelism)
</span></span><span style=display:flex><span>				}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>				procs := <span style=color:#fff;font-weight:700>uint32</span>(<span style=color:#ff0;font-weight:700>0</span>)
</span></span><span style=display:flex><span>				b.RunParallel(<span style=color:#fff;font-weight:700>func</span>(pb *testing.PB) {
</span></span><span style=display:flex><span>					chanidx := atomic.LoadUint32(&amp;procs)
</span></span><span style=display:flex><span>					<span style=color:#fff;font-weight:700>for</span> {
</span></span><span style=display:flex><span>						<span style=color:#fff;font-weight:700>if</span> atomic.CompareAndSwapUint32(&amp;procs, chanidx, chanidx+<span style=color:#ff0;font-weight:700>1</span>) {
</span></span><span style=display:flex><span>							<span style=color:#fff;font-weight:700>break</span>
</span></span><span style=display:flex><span>						}
</span></span><span style=display:flex><span>						chanidx = atomic.LoadUint32(&amp;procs)
</span></span><span style=display:flex><span>					}
</span></span><span style=display:flex><span>					chanidx = chanidx / <span style=color:#fff;font-weight:700>uint32</span>(parallelism)
</span></span><span style=display:flex><span>					<span style=color:#fff;font-weight:700>for</span> pb.Next() {
</span></span><span style=display:flex><span>						workWithChan(chans[chanidx])
</span></span><span style=display:flex><span>					}
</span></span><span style=display:flex><span>				})
</span></span><span style=display:flex><span>			})
</span></span><span style=display:flex><span>			b.Run(<span style=color:#0ff;font-weight:700>&#34;chan per goroutine&#34;</span>, <span style=color:#fff;font-weight:700>func</span>(b *testing.B) {
</span></span><span style=display:flex><span>				b.SetParallelism(parallelism)
</span></span><span style=display:flex><span>				numChans := runtime.GOMAXPROCS(<span style=color:#ff0;font-weight:700>0</span>) * parallelism
</span></span><span style=display:flex><span>				chans := <span style=color:#fff;font-weight:700>make</span>([]<span style=color:#fff;font-weight:700>chan</span> <span style=color:#fff;font-weight:700>int</span>, numChans)
</span></span><span style=display:flex><span>				<span style=color:#fff;font-weight:700>for</span> i := <span style=color:#ff0;font-weight:700>0</span>; i &lt; numChans; i++ {
</span></span><span style=display:flex><span>					chans[i] = <span style=color:#fff;font-weight:700>make</span>(<span style=color:#fff;font-weight:700>chan</span> <span style=color:#fff;font-weight:700>int</span>, workSize)
</span></span><span style=display:flex><span>				}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>				procs := <span style=color:#fff;font-weight:700>uint32</span>(<span style=color:#ff0;font-weight:700>0</span>)
</span></span><span style=display:flex><span>				b.RunParallel(<span style=color:#fff;font-weight:700>func</span>(pb *testing.PB) {
</span></span><span style=display:flex><span>					chanidx := atomic.LoadUint32(&amp;procs)
</span></span><span style=display:flex><span>					<span style=color:#fff;font-weight:700>for</span> {
</span></span><span style=display:flex><span>						<span style=color:#fff;font-weight:700>if</span> atomic.CompareAndSwapUint32(&amp;procs, chanidx, chanidx+<span style=color:#ff0;font-weight:700>1</span>) {
</span></span><span style=display:flex><span>							<span style=color:#fff;font-weight:700>break</span>
</span></span><span style=display:flex><span>						}
</span></span><span style=display:flex><span>						chanidx = atomic.LoadUint32(&amp;procs)
</span></span><span style=display:flex><span>					}
</span></span><span style=display:flex><span>					<span style=color:#fff;font-weight:700>for</span> pb.Next() {
</span></span><span style=display:flex><span>						workWithChan(chans[chanidx])
</span></span><span style=display:flex><span>					}
</span></span><span style=display:flex><span>				})
</span></span><span style=display:flex><span>			})
</span></span><span style=display:flex><span>		})
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Here we have 3 variants - one pool shared between all the goroutines, one pool per each goroutine and one pool per each CPU core while additional parameter parallelism is equal to number of goroutines per CPU core.</p><p>Here are the results with some omissions and reformatting to simplify reading.</p><div class=highlight><pre tabindex=0 style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Benchmark_Chan/Parallelism_1/one_chan                  60868 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_1/chan_per_core              3594 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_1/chan_per_goroutine         3584 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_2/one_chan                  61708 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_2/chan_per_core              4758 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_2/chan_per_goroutine         3328 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_3/one_chan                  62023 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_3/chan_per_core              5119 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_3/chan_per_goroutine         3243 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_4/one_chan                  63873 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_4/chan_per_core              5343 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_4/chan_per_goroutine         3375 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_5/one_chan                  63416 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_5/chan_per_core              5749 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_5/chan_per_goroutine         3319 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_10/one_chan                 63559 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_10/chan_per_core             6549 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_10/chan_per_goroutine        3243 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_20/one_chan                 64031 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_20/chan_per_core             7948 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_20/chan_per_goroutine        3230 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_50/one_chan                 64367 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_50/chan_per_core            14881 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_50/chan_per_goroutine        3285 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_100/one_chan                64066 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_100/chan_per_core           30397 ns/op
</span></span><span style=display:flex><span>Benchmark_Chan/Parallelism_100/chan_per_goroutine       3273 ns/op
</span></span></code></pre></div><p>So with single channel for all goroutines we immediately recieve enourmous contention that is nearly equal with any number of goroutines per CPU core.</p><p>With separate channel per each goroutine there is no contention obviously and no overhead from it.</p><p>With a channel per CPU core we start seeing the larger overhead the more goroutines we launch per CPU core.</p><p>Together with the previous post I can make an assumption that contention on some single sync.Pool may not become a bottleneck in a real world application, while contention on a single channel may cause some degradation.</p></div><footer></footer></article></section></div><footer class=footer><section class=container>Â©
2011 -
2023
Ivan Sopov
Â·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=/js/coder.min.9cf2dbf9b6989ef8eae941ffb4231c26d1dc026bca38f1d19fdba50177d8a9ac.js integrity="sha256-nPLb+baYnvjq6UH/tCMcJtHcAmvKOPHRn9ulAXfYqaw="></script></body></html>